# extended_lstm.py
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from typing import Optional, Tuple

# ---------------------------
# Utilities
# ---------------------------
def glorot_init(tensor: Tensor):
    if tensor is None: return
    fan_in, fan_out = tensor.size(0), tensor.size(1) if tensor.dim()==2 else tensor.numel()
    bound = math.sqrt(6.0 / (fan_in + fan_out))
    nn.init.uniform_(tensor, -bound, bound)

# ---------------------------
# 1) Hypergraph-Memory LSTM Cell (hgLSTM)
# ---------------------------
class HypergraphMemoryLSTMCell(nn.Module):
    """
    hgLSTM cell with a learnable adjacency matrix A_t (dynamic hypergraph).
    Node messages (m) updated by a GRU-style node updater using summed messages from neighbors.
    Simplified integration with an external feature x_t producing node inputs.
    """
    def __init__(self, input_size: int, hidden_size: int, num_nodes: int = 4, adj_rank: int = 16):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_nodes = num_nodes

        # project input to node-wise features
        self.input_proj = nn.Linear(input_size, num_nodes * hidden_size)

        # adjacency parametrisation: low-rank factorization A = U V^T (num_nodes x r)
        self.adj_r = min(adj_rank, num_nodes)
        self.U = nn.Parameter(torch.randn(num_nodes, self.adj_r) * 0.1)
        self.V = nn.Parameter(torch.randn(num_nodes, self.adj_r) * 0.1)
        # optional sparsity bias (learnable)
        self.adj_bias = nn.Parameter(torch.zeros(num_nodes, num_nodes))

        # node updater (GRU-style) for message vector m_i
        self.node_gru = nn.GRUCell(hidden_size, hidden_size)

        # readout to output hidden representation
        self.readout = nn.Linear(num_nodes * hidden_size, hidden_size)
        glorot_init(self.readout.weight)

    def forward(self, x_t: Tensor, prev_m: Optional[Tensor] = None
               ) -> Tuple[Tensor, Tensor]:
        """
        Args:
            x_t: (batch, input_size)
            prev_m: (batch, num_nodes, hidden_size) or None
        Returns:
            h_out: (batch, hidden_size)
            m_t: (batch, num_nodes, hidden_size)
        """
        B = x_t.size(0)
        if prev_m is None:
            prev_m = torch.zeros(B, self.num_nodes, self.hidden_size, device=x_t.device)

        # input -> per-node features
        node_in = self.input_proj(x_t)  # (B, num_nodes*hidden)
        node_in = node_in.view(B, self.num_nodes, self.hidden_size)

        # build adjacency (dense) via low-rank factorization
        A = self.U @ self.V.t()  # (num_nodes, num_nodes)
        A = A + self.adj_bias
        # sparsity penalty can be added externally (L1)
        A = torch.tanh(A)  # keep bounded; other options: relu, sigmoid

        # message passing: m_agg_i = sum_j A_{ij} * m_j
        # prev_m: (B, num_nodes, hidden)
        A_exp = A.unsqueeze(0)  # (1, n, n)
        m_agg = torch.matmul(A_exp, prev_m)  # (B, n, hidden)

        # node update (GRUCell per node)
        m_t = []
        for i in range(self.num_nodes):
            m_i = self.node_gru(node_in[:, i, :], m_agg[:, i, :])
            m_t.append(m_i)
        m_t = torch.stack(m_t, dim=1)  # (B, n, hidden)

        h_out = torch.tanh(self.readout(m_t.view(B, -1)))
        return h_out, m_t

# ---------------------------
# 2) Neural-Oscillator LSTM Cell (noLSTM)
# ---------------------------
class NeuralOscillatorLSTMCell(nn.Module):
    """
    A recurrent cell combining phase (phi) and amplitude gating (a_t).
    Inspired by the oscillator equations: phi_{t} = phi_{t-1} + omega + tanh(Wx + Uh)
    c_t = a_t * cos(phi_t) + (1-a_t) * c_{t-1}
    Returns new hidden h_t and cell c_t
    """
    def __init__(self, input_size: int, hidden_size: int, omega_init: float = 0.1):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.W_a = nn.Linear(input_size, hidden_size)
        self.U_a = nn.Linear(hidden_size, hidden_size, bias=False)
        self.b_a = nn.Parameter(torch.zeros(hidden_size))

        self.W_phi = nn.Linear(input_size, hidden_size)
        self.U_phi = nn.Linear(hidden_size, hidden_size, bias=False)
        # global frequency parameter per dimension (learnable)
        self.omega = nn.Parameter(torch.ones(hidden_size) * omega_init)

        # output projection
        self.h_proj = nn.Linear(hidden_size, hidden_size)

    def forward(self, x_t: Tensor, states: Optional[Tuple[Tensor, Tensor]] = None
               ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        """
        states: (c_{t-1}, phi_{t-1}) both (batch, hidden)
        Returns: h_t, (c_t, phi_t)
        """
        B = x_t.size(0)
        if states is None:
            c_prev = torch.zeros(B, self.hidden_size, device=x_t.device)
            phi_prev = torch.zeros(B, self.hidden_size, device=x_t.device)
        else:
            c_prev, phi_prev = states

        # amplitude gate
        a_t = torch.sigmoid(self.W_a(x_t) + self.U_a(torch.tanh(c_prev)) + self.b_a)

        # phase update
        phi_t = phi_prev + self.omega.unsqueeze(0) + torch.tanh(self.W_phi(x_t) + self.U_phi(torch.tanh(c_prev)))

        # cell update using cosine of phase
        c_t = a_t * torch.cos(phi_t) + (1.0 - a_t) * c_prev

        # hidden/output
        h_t = torch.tanh(self.h_proj(torch.tanh(c_t)))
        return h_t, (c_t, phi_t)

# ---------------------------
# 3) Low-Rank + Sparse Matrix Memory (utility wrapper)
# ---------------------------
class LowRankSparseMemory(nn.Module):
    """
    Holds low-rank U_t V_t^T and a sparse matrix S_t (soft-thresholded).
    Provides efficient read/write updates.
    """
    def __init__(self, mem_dim: int, rank: int = 8, sparsity_lambda: float = 1e-3):
        super().__init__()
        self.mem_dim = mem_dim
        self.rank = rank
        self.sparsity_lambda = sparsity_lambda

        # factors for low-rank memory: U (mem_dim x r), V (mem_dim x r)
        self.U = nn.Parameter(torch.randn(mem_dim, rank) * 0.01)
        self.V = nn.Parameter(torch.randn(mem_dim, rank) * 0.01)
        # sparse matrix stored as full but encouraged to be sparse via soft-thresholding
        self.S = nn.Parameter(torch.zeros(mem_dim, mem_dim))

    def read(self, key: Tensor) -> Tensor:
        """
        Key: (batch, mem_dim). Read returns (batch, mem_dim) representation.
        read = key @ (U V^T + S)
        """
        M_low = self.U @ self.V.t()  # (D, D)
        M = M_low + self.S
        return key @ M  # (batch, D)

    def update(self, delta_U: Tensor, delta_V: Tensor, delta_S: Tensor):
        """
        Apply additive updates. delta_* shapes must match.
        Soft-threshold S after update (proximal operator for L1).
        """
        self.U.data += delta_U.data
        self.V.data += delta_V.data
        self.S.data += delta_S.data

        # soft-threshold S towards sparsity
        with torch.no_grad():
            S = self.S.data
            self.S.data = torch.sign(S) * F.relu(torch.abs(S) - self.sparsity_lambda)

# ---------------------------
# 4) Contrastive Memory Regularized LSTM (CM-LSTM)
# ---------------------------
class ContrastiveMemoryLSTMCell(nn.Module):
    """
    Simple LSTMCell augmented with a contrastive memory buffer.
    The cell returns standard h_t and stores c_t vector in a memory buffer for contrastive loss.
    """
    def __init__(self, input_size: int, hidden_size: int, memory_pool_size: int = 128, temp: float = 0.1):
        super().__init__()
        self.cell = nn.LSTMCell(input_size, hidden_size)
        self.hidden_size = hidden_size

        # contrastive memory (non-learned buffer), stored as queue
        self.pool_size = memory_pool_size
        self.register_buffer("memory_queue", torch.zeros(memory_pool_size, hidden_size))
        self.queue_ptr = 0
        self.temp = temp

    def forward(self, x_t: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None
               ) -> Tuple[Tensor, Tensor]:
        """
        x_t: (batch, input_size)
        hx: (h_prev, c_prev) each (batch, hidden)
        returns: h_t, c_t
        """
        if hx is None:
            B = x_t.size(0)
            h_prev = torch.zeros(B, self.hidden_size, device=x_t.device)
            c_prev = torch.zeros(B, self.hidden_size, device=x_t.device)
        else:
            h_prev, c_prev = hx
        h_t, c_t = self.cell(x_t, (h_prev, c_prev))
        return (h_t, c_t)

    def enqueue_memory(self, vecs: Tensor):
        """Add vectors to the memory queue (simple FIFO) vecs: (k, D)"""
        k = vecs.size(0)
        for i in range(k):
            self.memory_queue[self.queue_ptr % self.pool_size] = vecs[i].detach().cpu()
            self.queue_ptr = (self.queue_ptr + 1) % self.pool_size

    def contrastive_loss(self, queries: Tensor, positives: Tensor) -> Tensor:
        """
        NT-Xent style loss using memory queue as negatives.
        queries: (batch, D), positives: (batch, D)
        """
        # normalize
        q = F.normalize(queries, dim=-1)
        p = F.normalize(positives, dim=-1)
        negatives = F.normalize(self.memory_queue.to(q.device), dim=-1)  # (pool_size, D)

        # positive logits
        pos_logits = torch.sum(q * p, dim=-1, keepdim=True) / self.temp  # (B, 1)
        # negative logits
        neg_logits = torch.matmul(q, negatives.t()) / self.temp  # (B, pool_size)
        logits = torch.cat([pos_logits, neg_logits], dim=1)  # (B, 1+pool)
        labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)
        loss = F.cross_entropy(logits, labels)
        return loss

# ---------------------------
# Example combined model skeleton
# ---------------------------
class ExtendedLSTMModel(nn.Module):
    """
    Example model that uses the modular cells above.
    You can swap cells or stack them to create experiments.
    """
    def __init__(self, input_size: int, hidden_size: int, 
                 num_classes: int = 2, num_nodes: int = 4):
        super().__init__()
        self.hgcell = HypergraphMemoryLSTMCell(input_size, hidden_size, num_nodes=num_nodes)
        self.nocell = NeuralOscillatorLSTMCell(input_size, hidden_size)
        self.lr_memory = LowRankSparseMemory(mem_dim=hidden_size, rank=min(8, hidden_size//2))
        self.cmcell = ContrastiveMemoryLSTMCell(input_size, hidden_size, memory_pool_size=256)

        # simple combiner
        self.classifier = nn.Sequential(
            nn.Linear(3 * hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x_seq: Tensor) -> Tuple[Tensor, dict]:
        """
        x_seq: (seq_len, batch, input_size)
        returns logits: (batch, num_classes), diagnostics dict
        """
        seq_len, B, _ = x_seq.shape
        # initial states
        m = None
        c = None
        phi = None
        h_cm = None
        c_cm = None

        hg_h = None
        no_h = None
        cm_h = None

        # accumulate final hidden states
        for t in range(seq_len):
            x_t = x_seq[t]
            hg_h, m = self.hgcell(x_t, m)       # (batch, hidden), m shape returned
            no_h, (c, phi) = self.nocell(x_t, (c, phi))
            (cm_h, c_cm) = self.cmcell(x_t, (h_cm, c_cm))
            h_cm = cm_h  # update for next step

            # optional: update low-rank memory by reading and writing
            read_vec = self.lr_memory.read(hg_h)  # (B, hidden)
            # compute deltas (toy - small gradient-like updates)
            delta_U = torch.zeros_like(self.lr_memory.U)
            delta_V = torch.zeros_like(self.lr_memory.V)
            delta_S = torch.zeros_like(self.lr_memory.S)
            # you can supply meaningful deltas from gradients or learned networks

            # self.lr_memory.update(delta_U, delta_V, delta_S)  # uncomment to update

        combined = torch.cat([hg_h, no_h, cm_h], dim=-1)
        logits = self.classifier(combined)

        diagnostics = {
            "hg_hidden": hg_h.detach().cpu() if hg_h is not None else None,
            "no_hidden": no_h.detach().cpu() if no_h is not None else None,
            "cm_hidden": cm_h.detach().cpu() if cm_h is not None else None
        }
        return logits, diagnostics

# ---------------------------
# Training loop stub / usage
# ---------------------------
if __name__ == "__main__":
    # Quick smoke-test / usage example
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seq_len = 20
    batch = 8
    input_dim = 32
    hidden = 64
    num_classes = 2

    model = ExtendedLSTMModel(input_dim, hidden, num_classes).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    ce = nn.CrossEntropyLoss()

    # dummy data
    x = torch.randn(seq_len, batch, input_dim, device=device)
    y = torch.randint(0, num_classes, (batch,), device=device)

    model.train()
    for epoch in range(3):
        optimizer.zero_grad()
        logits, diag = model(x)
        loss = ce(logits, y)

        # optional contrastive loss addition (example)
        # build positive pairs from cm hidden and hg hidden
        cm_hidden = diag["cm_hidden"].to(device)
        hg_hidden = diag["hg_hidden"].to(device)
        # if using a contrastive cell:
        # contr_loss = model.cmcell.contrastive_loss(cm_hidden, hg_hidden)
        # loss = loss + 0.1 * contr_loss

        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch} loss {loss.item():.4f}")

    print("SMOKE TEST COMPLETE")
